name: docker-test

on:
  push:
    paths-ignore:
      - '**.md'
      - 'website/**'
  pull_request:
    paths-ignore:
      - '**.md'
      - 'website/**'
  workflow_dispatch:
    inputs:
      # there is an outage for gha cache - disabling for now
      enable_cache:
        description: 'Enable build cache'
        type: boolean
        default: false
        required: false

permissions: read-all

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  test:
    runs-on: ${{ matrix.arch == 'arm64' && 'github-arm64-2c-8gb' || 'ubuntu-latest-16-cores' }}
    timeout-minutes: 240
    strategy:
      fail-fast: false
      matrix:
        backend:
          - llama
          - llama-cuda # this tests cpu inference with cuda libraries present in the image
        arch:
          - amd64
          - arm64
        exclude:
          - backend: llama-cuda
            arch: arm64
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@e3f713f2d8f53843e71c69a996d56f51aa9adfb9 # v2.14.1
        with:
          egress-policy: audit
          allowed-endpoints: >
            auth.docker.io:443
            huggingface.co:443
            *.huggingface.co:443
            *.hf.co:443
            cdn.dl.k8s.io:443
            dl.k8s.io:443
            download.docker.com:443
            gcr.io:443
            github.com:443
            *.githubusercontent.com:443
            production.cloudflare.docker.com:443
            proxy.golang.org:443
            registry-1.docker.io:443
            storage.googleapis.com:443
            *.blob.core.windows.net:443
            *.azureedge.net:443
            developer.download.nvidia.com:443
            dl-cdn.alpinelinux.org:443
            *.ubuntu.com:80
            ghcr.io:443
            sum.golang.org:443
            quay.io:443

      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2

      # need containerd image store for testing local images
      - uses: crazy-max/ghaction-setup-docker@e43656e248c0bd0647d3f5c195d116aacf6fcaf4 # v4.7.0
        with:
          version: version=v27.5.1
          daemon-config: |
            {
              "debug": true,
              "features": {
                "containerd-snapshotter": true
              }
            }
      - uses: crazy-max/ghaction-github-runtime@3cb05d89e1f492524af3d41a1c98c83bc3025124 # v3.1.0
      - uses: docker/setup-qemu-action@c7c53464625b32c7a7e944ae62b3e17d2b600130 # v3.7.0
        if: ${{ matrix.arch == 'arm64' }}

      - name: build aikit
        run: |
          CACHE_FLAGS=""
          if [ "${{ github.event.inputs.enable_cache }}" = "true" ]; then
            CACHE_FLAGS="--cache-from=type=gha,scope=aikit-${{ matrix.arch }} --cache-to=type=gha,scope=aikit-${{ matrix.arch }},mode=max"
          fi
          docker buildx build . -t aikit:test \
            --load --provenance=false --progress plain \
            $CACHE_FLAGS \
            --platform linux/${{ matrix.arch }}

      - name: build test model
        run: |
          CACHE_FLAGS=""
          if [ "${{ github.event.inputs.enable_cache }}" = "true" ]; then
            CACHE_FLAGS="--cache-from=type=gha,scope=testmodel-${{ matrix.arch }} --cache-to=type=gha,scope=testmodel-${{ matrix.arch }},mode=max"
          fi
          docker buildx build . -t testmodel:test \
            -f test/aikitfile-${{ matrix.backend }}.yaml \
            --load --provenance=false --progress plain \
            $CACHE_FLAGS \
            --platform linux/${{ matrix.arch }} \
            ${{ matrix.backend == 'llama-cuda' && '--build-arg=runtime=cuda' || '' }}

      - name: list images
        run: docker images

      - name: run test model
        run: docker run --name testmodel -d -p 8080:8080 --platform "linux/${{ matrix.arch }}" testmodel:test

      - name: run llama test
        run: |
          result=$(curl --fail --retry 10 --retry-all-errors http://127.0.0.1:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
            "model": "llama-3.2-1b-instruct",
            "messages": [{"role": "user", "content": "explain kubernetes in a sentence"}]
          }')
          echo $result

          choices=$(echo "$result" | jq '.choices')
          if [ -z "$choices" ]; then
            exit 1
          fi

      - name: run tool calling test
        run: |
          result=$(curl --fail --retry 10 --retry-all-errors http://127.0.0.1:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
            "model": "llama-3.2-1b-instruct",
            "messages": [{"role": "user", "content": "What is the weather in Paris?"}],
            "tools": [{
              "type": "function",
              "function": {
                "name": "get_weather",
                "description": "Get the current weather for a location",
                "parameters": {
                  "type": "object",
                  "properties": {
                    "location": {"type": "string", "description": "The city name"}
                  },
                  "required": ["location"]
                }
              }
            }]
          }')
          echo "$result"

          tool_calls=$(echo "$result" | jq -r '.choices[0].message.tool_calls')
          if [ "$tool_calls" = "null" ] || [ -z "$tool_calls" ]; then
            echo "ERROR: No tool_calls in response"
            exit 1
          fi

          echo "Tool calling test passed"

      - name: save logs
        if: always()
        run: docker logs testmodel > /tmp/docker-${{ matrix.backend }}-${{ matrix.arch }}.log

      - name: publish test artifacts
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: test-${{ matrix.arch }}-${{ matrix.backend }}
          path: |
            /tmp/*.log
            /tmp/images/*.png

  test-protocol:
    runs-on: ubuntu-latest-16-cores
    timeout-minutes: 240
    strategy:
      fail-fast: false
      matrix:
        protocol:
          - oci
          - hf
    steps:
      - name: Harden Runner
        uses: step-security/harden-runner@e3f713f2d8f53843e71c69a996d56f51aa9adfb9 # v2.14.1
        with:
          egress-policy: audit
          allowed-endpoints: >
            auth.docker.io:443
            huggingface.co:443
            *.huggingface.co:443
            *.hf.co:443
            cdn.dl.k8s.io:443
            dl.k8s.io:443
            download.docker.com:443
            gcr.io:443
            github.com:443
            *.githubusercontent.com:443
            production.cloudflare.docker.com:443
            proxy.golang.org:443
            registry-1.docker.io:443
            storage.googleapis.com:443
            *.blob.core.windows.net:443
            *.azureedge.net:443
            *.ubuntu.com:80
            developer.download.nvidia.com:443
            dl-cdn.alpinelinux.org:443
            registry.ollama.ai:443
            *.cloudflarestorage.com:443
            ghcr.io:443
            sum.golang.org:443
            quay.io:443

      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2

      # need containerd image store for testing local images
      - uses: crazy-max/ghaction-setup-docker@e43656e248c0bd0647d3f5c195d116aacf6fcaf4 # v4.7.0
        with:
          version: version=v27.5.1
          daemon-config: |
            {
              "debug": true,
              "features": {
                "containerd-snapshotter": true
              }
            }
      - uses: crazy-max/ghaction-github-runtime@3cb05d89e1f492524af3d41a1c98c83bc3025124 # v3.1.0

      - name: build aikit
        run: |
          CACHE_FLAGS=""
          if [ "${{ github.event.inputs.enable_cache }}" = "true" ]; then
            CACHE_FLAGS="--cache-from=type=gha,scope=aikit-amd64 --cache-to=type=gha,scope=aikit-amd64,mode=max"
          fi
          docker buildx build . -t aikit:test \
            --load --provenance=false --progress plain \
            $CACHE_FLAGS

      - name: build test model
        run: |
          CACHE_FLAGS=""
          if [ "${{ github.event.inputs.enable_cache }}" = "true" ]; then
            CACHE_FLAGS="--cache-from=type=gha,scope=testmodel-${{ matrix.protocol }} --cache-to=type=gha,scope=testmodel-${{ matrix.protocol }},mode=max"
          fi
          docker buildx build . -t testmodel:test \
            -f test/aikitfile-${{ matrix.protocol }}.yaml \
            --load --provenance=false --progress plain \
            $CACHE_FLAGS

      - name: list images
        run: docker images

      - name: run test model
        run: docker run --name testmodel -d -p 8080:8080 testmodel:test

      - name: run llama test
        run: |
          result=$(curl --fail --retry 10 --retry-all-errors http://127.0.0.1:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
            "model": "llama-3.2-1b-instruct",
            "messages": [{"role": "user", "content": "explain kubernetes in a sentence"}]
          }')
          echo $result

          choices=$(echo "$result" | jq '.choices')
          if [ -z "$choices" ]; then
            exit 1
          fi

      - name: save logs
        if: always()
        run: docker logs testmodel > /tmp/docker-${{ matrix.protocol }}.log

      - name: publish test artifacts
        if: always()
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: test-${{ matrix.protocol }}
          path: |
            /tmp/*.log
